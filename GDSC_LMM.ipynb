{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nmMPr_caDfW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Log in to Hugging Face\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)   # Gpt2 based tokenizer\n"
      ],
      "metadata": {
        "id": "Ezhf8H5Ga3pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load wikitext dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "# Preview a sample\n",
        "print(dataset[\"train\"][50])\n"
      ],
      "metadata": {
        "id": "uGMX0SvubCsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset # Import load_dataset\n",
        "\n",
        "# Add the padding token to the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as pad token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Load wikitext dataset if not already loaded\n",
        "try:\n",
        "    dataset\n",
        "except NameError:\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True) # dataset is now accessible\n"
      ],
      "metadata": {
        "id": "mKzkWV-Tbz3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the pad_token_id to be the same as eos_token_id\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"How to do multiplication\"\n",
        "\n",
        "# Tokenizing input with padding and generating attention mask\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True) ## Embedding\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=50)\n",
        "\n",
        "# Decode and print the output\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "_5XgOzD4douE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "gh8qZElOIsLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting to Hugging Face"
      ],
      "metadata": {
        "id": "B0QpO3_CNGvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the model to Hugging Face Hub\n",
        "model.push_to_hub(\"my-awesome-llm\")\n",
        "tokenizer.push_to_hub(\"my-awesome-llm\")\n"
      ],
      "metadata": {
        "id": "7YK6HNsRd9AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing our Tokenizer and testing"
      ],
      "metadata": {
        "id": "sNP4GF_vNAna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer from the model hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rveeee/my-awesome-llm\")\n",
        "\n",
        "# Test tokenization\n",
        "sample_text = \"Hello, how are you?\"\n",
        "tokens = tokenizer(sample_text)\n",
        "print(\"Token IDs:\", tokens['input_ids'])\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n"
      ],
      "metadata": {
        "id": "Qo3XG2GJf1zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing our Model and Testing"
      ],
      "metadata": {
        "id": "lE6l0czyNwM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rveeee/my-awesome-llm\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rveeee/my-awesome-llm\")\n",
        "\n",
        "# Prepare input text\n",
        "input_text = \"Once upon a time\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Include the attention mask\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=50,\n",
        "    pad_token_id=tokenizer.eos_token_id,  # Set pad token ID\n",
        "    attention_mask=attention_mask  # Pass the attention mask\n",
        ")\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "aMahpZuvgS50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"rveeee/my-awesome-llm\")"
      ],
      "metadata": {
        "id": "d8OnlTA_g46k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rveeee/my-awesome-llm\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rveeee/my-awesome-llm\")"
      ],
      "metadata": {
        "id": "u2oXuBRkg7AR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}