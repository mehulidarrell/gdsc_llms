{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuXIFTFapAMI"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes  # Install bitsandbytes for efficient model inference\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git  # Install the latest version of Hugging Face's transformers library\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git  # Install the latest version of the PEFT (Parameter-Efficient Fine-Tuning) library\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git  # Install the latest version of Hugging Face's accelerate library for multi-GPU support\n",
        "!pip install -q datasets  # Install the datasets library for loading and managing datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's load the model we are going to use - GPT-neo-x-20B! Note that the model itself is around 40GB in half precision"
      ],
      "metadata": {
        "id": "MJ-5idQwzvg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"EleutherAI/gpt-neox-20b\"#32 bit to 4 bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",#Normalized Float 4 (NF4), a quantization technique used in the context of reducing the precision of floating-point numbers\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "E0Nl5mWL0k2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ],
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()#tradeoff memory using backpropogation,by selectively discarding some of these intermediate activations during the forward pass and recomputing them during the backward pass.\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()#returns total number of elements in the input tensor\n",
        "        if param.requires_grad:#if it is trainable or not grad for gradient\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "gkIcwsSU01EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model#lOW RANK ADAPTATION\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,#Rank of the updated matrices\n",
        "    lora_alpha=32,#scales the learned weights\n",
        "    target_modules=[\"query_key_value\"],#Target specific modules to apply LoRA\n",
        "    lora_dropout=0.05,#Prevents overfitting and accuracy\n",
        "    bias=\"none\", # Determines whether to include bias in adaptation\n",
        "    task_type=\"CAUSAL_LM\"# Specifies the task type (e.g., causal language modeling)\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "Ybeyl20n3dYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a common dataset, english quotes, to fine tune our model on famous quotes."
      ],
      "metadata": {
        "id": "FCc64bfnmd3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
      ],
      "metadata": {
        "id": "s6f4z8EYmcJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."
      ],
      "metadata": {
        "id": "_0MOtwf3zdZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Set the padding token of the tokenizer to be the same as the end-of-sequence (EOS) token.\n",
        "# This is important when padding sequences to make sure the model knows where the end of each sequence is.\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    # Initialize a Trainer object to handle the model's training process, simplifying the fine-tuning of transformers models.\n",
        "\n",
        "    model=model,\n",
        "    # Specify the model that will be trained. This is likely a pre-trained model that you're fine-tuning.\n",
        "\n",
        "    train_dataset=data[\"train\"],\n",
        "    # Provide the training dataset for the trainer. It expects a dictionary where \"train\" is the key for the training data.\n",
        "\n",
        "    args=transformers.TrainingArguments(\n",
        "        # Define the arguments for training the model using the TrainingArguments class.\n",
        "\n",
        "        per_device_train_batch_size=1,\n",
        "        # Set the batch size per device (e.g., per GPU) to 1, meaning one sample will be processed at a time per device.\n",
        "\n",
        "        gradient_accumulation_steps=4,\n",
        "        # Perform gradient accumulation over 4 steps, effectively simulating a larger batch size by updating the model weights\n",
        "        # after every 4 forward passes. This helps when using small batch sizes to avoid memory issues.\n",
        "\n",
        "        warmup_steps=2,\n",
        "        # Number of warmup steps where the learning rate gradually increases from 0 to the specified learning rate.\n",
        "        # This helps stabilize training at the beginning.\n",
        "\n",
        "        max_steps=10,\n",
        "        # Set the maximum number of training steps to 10. Training will stop after this many steps regardless of the number of epochs.\n",
        "\n",
        "        learning_rate=2e-4,\n",
        "        # Set the learning rate to 0.0002, which controls how much the model's weights are adjusted with each update.\n",
        "        # Lower values are safer but slower to converge.\n",
        "\n",
        "        fp16=True,\n",
        "        # Enable mixed precision training with 16-bit floating point (FP16) to speed up training and reduce memory usage.\n",
        "\n",
        "        logging_steps=1,\n",
        "        # Log metrics (such as loss) every 1 step during training to monitor progress.\n",
        "\n",
        "        output_dir=\"outputs\",\n",
        "        # Specify the directory where the model checkpoints, logs, and other outputs will be saved.\n",
        "\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "        # Use the paged AdamW optimizer with 8-bit precision to reduce memory usage while maintaining efficient optimization.\n",
        "    ),\n",
        "\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    # Use a data collator that dynamically pads inputs for language modeling.\n",
        "    # mlm=False indicates that this is not a masked language modeling task (as in BERT), but rather a causal language modeling task.\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "# Disable caching for the model during training to avoid unnecessary warnings.\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "jq0nX33BmfaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEv87NUXalYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputs\")"
      ],
      "metadata": {
        "id": "p66mZk1RAlOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig.from_pretrained('outputs')\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "L2Hllu-bCuN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Mother Teresa\"\n",
        "device = \"cuda:0\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "T1TiIH6vAlr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kEESIVXyESi-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}